# @package _global_

model:
  use_lora: true
  lora_config:
    target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "down_proj", "up_proj"]
    lora_alpha: 32
    lora_dropout: 0.05
    r: 32
    bias: "none"
    task_type: "CAUSAL_LM"

trainer:
  args:
    per_device_train_batch_size: 4
    gradient_accumulation_steps: 8
    learning_rate: 1e-4
    num_train_epochs: 5
    lr_scheduler_type: constant
    logging_steps: 10
    save_total_limit: 1
  

