use_lora: true
quantization_config: null # qlora, 4bit, 8bit
model_args:
  pretrained_model_name_or_path: "meta-llama/Llama-2-7b-hf"
  attn_implementation: 'flash_attention_2'
tokenizer_args:
  pretrained_model_name_or_path: "meta-llama/Llama-2-7b-hf"
lora_config:
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "down_proj", "up_proj", "lm_head"]
  lora_alpha: 128
  lora_dropout: 0.05
  r: 128
  bias: "none"
  task_type: "CAUSAL_LM"
template_args:  # Used in creating prompts for the dataset. See src/data/utils.py#preprocess_chat_instance.
  apply_chat_template: False
  user_start_tag: "Question: "
  user_end_tag: "\n"
  asst_start_tag: "Answer: "
  asst_end_tag: "\n\n"